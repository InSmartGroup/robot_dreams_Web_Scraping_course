{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Parser Project Presentation\n",
    "<hr>"
   ],
   "id": "8153eea129c515b3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "from Project.Parser.parser import Parser",
   "id": "aa2fa7d83a84c845",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# define the needed URL\n",
    "URL = r\"https://www.tmforum.org/membership/current-members/\"\n",
    "\n",
    "# and create the Parser object\n",
    "parser = Parser(url=URL)"
   ],
   "id": "56b6f61be37b4fdd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# the parser extracts the domain name from the given URL\n",
    "print(parser.domain_name)"
   ],
   "id": "fdfad8d14c17b7b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# we can check the request status\n",
    "parser.check_request_status()"
   ],
   "id": "68dbe2a1a362204c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Parsing websites\n",
    "<hr>"
   ],
   "id": "91df2848916d3088"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# we can define the preferred parser\n",
    "# and also save the website's HTML to an HTML file to avoid sending too many requests to the server\n",
    "filename = \"parsing_result_1.html\"\n",
    "\n",
    "parser.parse(save_to_file=filename)"
   ],
   "id": "60c384a14d6a5ad1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# the parsed data is also stored in a clean HTML form of the Parser class attribute and can be used separately\n",
    "print(parser.content)"
   ],
   "id": "c71d1714768e2821",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# we can read the file with our parsing results\n",
    "website_content = parser.read_from_file(filename)\n",
    "\n",
    "print(website_content)"
   ],
   "id": "983a7ee68b422f9b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Parsing content with Beautifulsoup logic\n",
    "<hr>"
   ],
   "id": "c563ed56436d105"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# the Parser class allows searching for the needed content using both the Beautifulsoup \"find\" and \"find_all\" methods\n",
    "# and the XPath logic\n",
    "\n",
    "# we can find either one specific element or all the elements by using the \"find_all\" attribute\n",
    "rows = parser.search_html(\"li\", {\"class\": \"span4 tmf-current-members-column\"}, find_all=True)\n",
    "\n",
    "# let's check how many elements we've got\n",
    "print(len(rows))"
   ],
   "id": "106776cc11b21069",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# and check several elements\n",
    "for i in rows[:3]:\n",
    "    print(i)"
   ],
   "id": "a6d9e236fe11e7c5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# first, let's find all the company names\n",
    "company_names = []\n",
    "\n",
    "for i in rows:\n",
    "    company_names.append(i.text.strip())\n",
    "    \n",
    "for i in company_names[:5]:\n",
    "    print(i)"
   ],
   "id": "ca81bae8dab040fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# secondly, we need to get all the URLs from those elements as well\n",
    "links = []\n",
    "\n",
    "for i in rows:\n",
    "    links.append(i.a[\"href\"])\n",
    "    \n",
    "for i in links[:5]:\n",
    "    print(i)"
   ],
   "id": "949cb64d4113b8b5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# we can come up with the right URLS by joining them with the \"domain_name\" attribute\n",
    "links = [f\"{parser.domain_name}{i}\" for i in links]\n",
    "    \n",
    "for i in links[:5]:\n",
    "    print(i)"
   ],
   "id": "202a510c565ee3a2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# finally, let's retrieve all the company websites\n",
    "# the XPath pattern has to be defined beforehand\n",
    "websites = parser.crawl(links[0:3], xpath_pattern=\"//p/a/@href\")\n",
    "\n",
    "for i in websites:\n",
    "    print(i)"
   ],
   "id": "15b7561062af011",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Parsing content with XPath logic\n",
    "<hr>"
   ],
   "id": "fe8f14bbebf5201d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# let's repeat everything, but this time with XPath logic\n",
    "# the XPatch pattern has to be defined beforehand\n",
    "# first, let's find company names\n",
    "names = parser.search_xpath(\"//li[@class='span4 tmf-current-members-column']/a/text()\")\n",
    "\n",
    "# and clean the names\n",
    "names = [i.strip() for i in names]\n",
    "\n",
    "# let's check how many names we've got\n",
    "print(len(names))"
   ],
   "id": "9743abd6b671f391",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# let's check several names\n",
    "for i in names[:5]:\n",
    "    print(i)"
   ],
   "id": "54730de57796cb3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# next, let's find the needed URLS\n",
    "links = parser.search_xpath(\"//li[@class='span4 tmf-current-members-column']/a/@href\")\n",
    "\n",
    "for i in links[:5]:\n",
    "    print(i)"
   ],
   "id": "37cff9edde54fae6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# join them with the domain name\n",
    "links = [f\"{parser.domain_name}{i.strip()}\" for i in links]\n",
    "\n",
    "for i in links[:5]:\n",
    "    print(i)"
   ],
   "id": "ad02f23c695d4f0c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# finally, let's crawl these links\n",
    "# it's also possible to set the sleep timer\n",
    "websites = parser.crawl(links[0:3], xpath_pattern=\"//p/a/@href\", sleep_timer=2)\n",
    "\n",
    "for i in websites:\n",
    "    print(i)"
   ],
   "id": "5ff47b1b52188b5c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# also, there's an option to retry the last operation performed by Parser (for instance, in case of errors)\n",
    "# all we need is to use the \"retry\" method\n",
    "website = parser.retry()\n",
    "\n",
    "for i in websites:\n",
    "    print(i)"
   ],
   "id": "d9d4019146422fad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Using random headers\n",
    "<hr>"
   ],
   "id": "f20324860c54df7a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Parser can randomize its headers upon its initialization\n",
    "# all the headers are stored in the \"headers.json\" file, which comes with the package\n",
    "# let's create several Parser instances to see that it's working properly\n",
    "for i in range(5):\n",
    "    parser = Parser(url=URL, random_headers=True)\n",
    "    print(parser.headers[\"User-Agent\"])"
   ],
   "id": "4bc824c5cfd53fca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# but we can also change headers of the existing parser if necessary\n",
    "# let's create a new Parser instance with default headers\n",
    "parser = Parser(url=URL, random_headers=False)\n",
    "print(parser.headers[\"User-Agent\"])\n",
    "\n",
    "# and apply the \"change_headers\" method to change the headers\n",
    "parser.change_headers()\n",
    "print(parser.headers[\"User-Agent\"])\n",
    "\n",
    "# Parser also supports mobile user agents\n",
    "parser.change_headers()\n",
    "print(parser.headers[\"User-Agent\"])"
   ],
   "id": "f5cf9812681d0d83",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### One more Parser test\n",
    "<hr>"
   ],
   "id": "f1cd9a368a30fed1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# now let's run a new test from scratch\n",
    "# this time, we'll be parsing user agents from the following URL\n",
    "URL = r\"https://www.useragents.me/\"\n",
    "\n",
    "parser = Parser(URL, random_headers=True)\n",
    "print(parser.domain_name)\n",
    "parser.check_request_status()\n",
    "\n",
    "# parse the website without saving the HTML content to a file\n",
    "content = parser.parse()\n",
    "\n",
    "# we'll parse the website using XPath\n",
    "xpath_pattern = r\"//div/textarea[@class='form-control ua-textarea']/text()\"\n",
    "user_agents = parser.search_xpath(xpath_pattern)\n",
    "\n",
    "# finally, let's check the result\n",
    "for i in user_agents:\n",
    "    print(i)"
   ],
   "id": "290a1dd00454d673",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Final Parser test\n",
    "<hr>"
   ],
   "id": "360f02a09e6b4b7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# here's another test for my Parser class\n",
    "# this time we'll be parsing speakers from some website that I found on the Internet\n",
    "URL = r\"https://www.vabio.org/about/board-members/\"\n",
    "\n",
    "parser = Parser(URL, random_headers=True)\n",
    "\n",
    "parser.parse()\n",
    "\n",
    "# retrieve the needed elements using Beautifulsoup logic\n",
    "elements = parser.search_html(\"section\", {\"class\": \"team\"})\n",
    "\n",
    "# get the list of full names\n",
    "names = elements.find_all(\"span\", {\"class\": \"member-name\"})\n",
    "names = [i.text for i in names]\n",
    "\n",
    "# get the list of people's job titles\n",
    "titles = elements.find_all(\"span\", {\"class\": \"member-title\"})\n",
    "titles = [i.text for i in titles]\n",
    "\n",
    "# get the list of company names\n",
    "companies = elements.find_all(\"span\", {\"class\": \"member-company\"})\n",
    "companies = [i.text for i in companies]\n",
    "\n",
    "# and get the list of personal LinkedIn profiles using XPath logic\n",
    "linkedin_links = parser.search_xpath(\"//div[@class='member-info']/a/@href\")\n",
    "\n",
    "# finally, save the result to a JSON file \n",
    "labels = [\"name\", \"title\", \"company\", \"LI_url\"]\n",
    "dataset = [names, titles, companies, linkedin_links]\n",
    "\n",
    "result = []\n",
    "\n",
    "index = 0\n",
    "for i in range(len(names)):\n",
    "    result.append({\n",
    "        \"name\": names[index],\n",
    "        \"title\": titles[index],\n",
    "        \"company_name\": companies[index],\n",
    "        \"linkedin_url\": linkedin_links[index]\n",
    "    })\n",
    "    index += 1\n",
    "\n",
    "parser.to_json(\"parsing_result_2.json\", result)\n",
    "\n",
    "# the end result can be checked in the \"parsing_result_2.json\" file"
   ],
   "id": "780d21f18e732ba9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Thank you for your attention!",
   "id": "aa53999a5bd6cf64"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "6182c75cb0110ae9",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
